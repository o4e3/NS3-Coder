{
  "repo_name": "wwydmanski/RLinWiFi",
  "github_url": "https://github.com/wwydmanski/RLinWiFi",
  "readme": "# RLinWiFi\nCode for the following research article:\n\n> W. Wydma\u0144ski and S. Szott, \"[Contention Window Optimization in IEEE 802.11ax Networks with Deep Reinforcement Learning](https://ieeexplore.ieee.org/document/9417575),\" 2021 IEEE Wireless Communications and Networking Conference (WCNC), 2021, doi: 10.1109/WCNC49053.2021.9417575.\n\nPreprint available on [Arxiv](https://arxiv.org/pdf/2003.01492).\n\nThe main focus of this work is exploiting a reinforcement learning agent for maximizing WiFi's throughput.\n\n## Prerequisites\nIn order to run this code you need python 3.6 (tensorflow dependency) with installed dependencies:\n```\nconda env create -f environment.yaml\n```\nAfter creating the conda env, and installing ns-3.29 **you need a working [ns3-gym](https://github.com/tkn-tub/ns3-gym) environment.** Ns3Gym python package is a part of a larger framework, so installing it on its own is, unfortunately, not enough.\n\n\n## Installation\nClone the repo so that `linear-mesh` directory lands directly in ns3's `scratch`. \n\n## Execution\nAll basic configuration of the agents can be done within the file `linear-mesh/agent_training.py` (DDPG) and `linear-mesh/tf_agent_training.py` (DQN).\n\nBenchmark of the static CW values and the original 802.11 backoff algorithm can be set up using `linear-mesh/beb.py` file:\n\n```\nusage: beb_tests.py [-h] [--scenario SCENARIOS [SCENARIOS ...]] [--beb]\n                    N [N ...]\n\nRun BEB tests\n\npositional arguments:\n  N                     number of stations for the scenario (min: 5)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --scenario SCENARIOS [SCENARIOS ...]\n                        scenarios to run (available: [basic, convergence])\n  --beb                 run 802.11 default instead of look-up table\n```\n\nExample:\n```bash\npython agent_training.py                                          # DDPG agent\npython tf_agent_training.py                                       # DQN agent\npython beb_tests.py --beb 5 10 15 --scenario basic convergence    # Original 802.11 backoff\n```\n\nExpected output:\n```\nSteps per episode: 6000\nWaiting for simulation script to connect on port: tcp://localhost:46417\nPlease start proper ns-3 simulation script using ./waf --run \"...\"\nWaf: Entering directory `/mnt/d/Programy/ns-allinone-3.29/ns-3.29/build'\nWaf: Leaving directory `/mnt/d/Programy/ns-allinone-3.29/ns-3.29/build'\nBuild commands will be stored in build/compile_commands.json\n'build' finished successfully (29.428s)\nNs3Env parameters:\n--nWifi: 6\n--simulationTime: 60\n--openGymPort: 46417\n--envStepTime: 0.01\n--seed: -1\n--agentType: continuous\n--scenario: convergence\n--dryRun: 0\nSimulation started\nSimulation process id: 20062 (parent (waf shell) id: 20045)\nWaiting for Python process to connect on port: tcp://localhost:46417\nPlease start proper Python Gym Agent\nObservation space shape: (1, 300)\nAction space shape: (1, 1)\nCuDNN version: 7102\ncpu\n\n0\n  3%|\u258e         | 182/6300 [00:16<09:22, 10.88it/s, curr_speed=0.00 Mbps, mb_sent=0.00 Mb]\n```\n\n## Reading results\nThe script saves results of the run in `logs/` directory.\n\nExample graphs of an experiment:\n![](https://i.imgur.com/g8hiAz9.png)\n\n## Referencing\nYou can cite this code as \n\n```\n@INPROCEEDINGS{wydmanski2021contention,\n  author={Wydma\u0144ski, Witold and Szott, Szymon},\n  booktitle={2021 IEEE Wireless Communications and Networking Conference (WCNC)}, \n  title={Contention Window Optimization in IEEE 802.11ax Networks with Deep Reinforcement Learning}, \n  year={2021},\n  volume={},\n  number={},\n  pages={1-6},\n  doi={10.1109/WCNC49053.2021.9417575}}\n\n```\n",
  "examples": []
}