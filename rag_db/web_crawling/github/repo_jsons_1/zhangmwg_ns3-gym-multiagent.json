{
  "repo_name": "zhangmwg/ns3-gym-multiagent",
  "github_url": "https://github.com/zhangmwg/ns3-gym-multiagent",
  "readme": "# ns3-gym for multi-agent\n\nMultiEnv is an extension of [ns3-gym](https://github.com/tkn-tub/ns3-gym), so that the nodes in the network can be completely regarded as independent agents, which have their own states, observations, and rewards. \n\nNOTE: We formalize the network problem as a multi-agent extension Markov decision processes (MDPs) called Partially Observable Markov Games (POMGs).\n\n**Why we use a multi-agent environment?**\n\n**Fully-distributed learning:** Algorithms with centralized learning process are not applicable in the real computer network. The centralized learning controller is usually unable to gather collected environment transitions from widely distributed routers once an action is executed somewhere and to update the parameters of each neural network simultaneously caused by the limited bandwidth. [FROM: You, Xinyu, et al. \"Toward Packet Routing with Fully-distributed Multi-agent Deep Reinforcement Learning.\" arXiv preprint arXiv:1905.03494 (2019).]\n\nCode Reference: Lowe R, Wu Y, Tamar A, et al. Multi-agent actor-critic for mixed cooperative-competitive environments[C]//Advances in neural information processing systems. 2017: 6379-6390. https://github.com/openai/multiagent-particle-envs\n\n## How (Python)\n```python\nfrom ns3gym import ns3_multiagent_env as ns3env\n```\n\n## How (ns-3)\n```C++\n///\\{ Each agent OpenGym Env \n  virtual Ptr<OpenGymSpace> GetActionSpace(uint32_t agent_id) = 0;\n  virtual Ptr<OpenGymSpace> GetObservationSpace(uint32_t agent_id) = 0;\n  virtual Ptr<OpenGymDataContainer> GetObservation(uint32_t agent_id) = 0;\n  virtual float GetReward(uint32_t agent_id) = 0;\n  virtual bool GetDone(uint32_t agent_id) = 0;\n  virtual std::string GetInfo(uint32_t agent_id) = 0;\n  virtual bool ExecuteActions(uint32_t agent_id, Ptr<OpenGymDataContainer> action) = 0;\n  ///\\}\n```\n## Example \n### multi-agent example 1\nThis example shows how to create an ns3-gym environment with multiple agents in one Python processes. Similar to the \n[multiagent-particle-envs](https://github.com/openai/multiagent-particle-envs)\n\n[multi-agent example 1](https://github.com/zhangmwg/ns3-gym-multiagent/tree/master/examples/multigym)\n\n### multi-agent example 2\nThis example shows how to create an ns3-gym environment with multiple agents and connects them to multiple independent Python processes.\n\n[multi-agent example 2](https://github.com/zhangmwg/ns3-gym-multiagent/tree/master/examples/multi-agent)\n\nns3-gym\n============\n\n[OpenAI Gym](https://gym.openai.com/) is a toolkit for reinforcement learning (RL) widely used in research. The network simulator [ns-3](https://www.nsnam.org/) is the de-facto standard for academic and industry studies in the areas of networking protocols and communication technologies. [ns3-gym](https://github.com/tkn-tub/ns3-gym) is a framework that integrates both OpenAI Gym and ns-3 in order to encourage usage of RL in networking research.\n\nInstallation\n============\n\nhttps://github.com/tkn-tub/ns3-gym\n\nHow to reference ns3-gym?\n============\n\nPlease use the following bibtex :\n\n```\n@inproceedings{ns3gym,\n  Title = {{ns-3 meets OpenAI Gym: The Playground for Machine Learning in Networking Research}},\n  Author = {Gaw{\\l}owicz, Piotr and Zubow, Anatolij},\n  Booktitle = {{ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM)}},\n  Year = {2019},\n  Location = {Miami Beach, USA},\n  Month = {November},\n  Url = {http://www.tkn.tu-berlin.de/fileadmin/fg112/Papers/2019/gawlowicz19_mswim.pdf}\n}\n```\n\n```\n@article{ns3gym,\n  author    = {Gawlowicz, Piotr and Zubow, Anatolij},\n  title     = {{ns3-gym: Extending OpenAI Gym for Networking Research}},\n  journal   = {CoRR},\n  year      = {2018},\n  url       = {https://arxiv.org/abs/1810.03943},\n  archivePrefix = {arXiv},\n}\n```\n",
  "examples": []
}